# chat.py - Đã sửa lỗi và thêm stream response, hỗ trợ đọc file PDF, CSV, DOCX, image với qwen2.5vl, thêm system prompt theo phong cách Xiaomi SU7 cho 4T với gender
from fastapi import APIRouter, Depends, HTTPException, File, UploadFile, Body
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from app.db import get_db
from app.models import ChatMessage as ModelChatMessage, Conversation as ModelConversation, User
from app.schemas import ChatMessageIn, Conversation, ConversationCreate, ConversationUpdate, ChatMessage, ChatMessageUpdate
from app.routers.task import get_current_user
import ollama
from ollama import web_search, web_fetch
import faiss
import numpy as np
import json
import os
from datetime import datetime
from typing import List, Optional, Dict, Any, Union
import logging
import base64
import PyPDF2
from docx import Document
import pandas as pd
import io
from concurrent.futures import ThreadPoolExecutor
from rank_bm25 import BM25Okapi
import re

logger = logging.getLogger(__name__)

router = APIRouter()
DIM = 768
executor = ThreadPoolExecutor(max_workers=2)

def get_embedding(text: str, max_length: int = 1024) -> np.ndarray:
    """Tạo embedding cho text, với max_length tăng để giữ nhiều context hơn."""
    try:
        if len(text) > max_length:
            text = text[:max_length]
        resp = ollama.embeddings(model="embeddinggemma:latest", prompt=text)
        return np.array(resp["embedding"])
    except Exception as e:
        logger.error(f"Lỗi khi tạo embedding từ Ollama: {e}")
        return np.zeros(DIM)

def extract_text_from_file(file_content: Union[bytes, str]) -> str:
    """Trích xuất nội dung từ file PDF, CSV, DOCX hoặc text."""
    if isinstance(file_content, str):
        try:
            file_content = base64.b64decode(file_content)
        except:
            return file_content

    def sync_extract(file_content: Union[bytes, str]) -> str:
        try:
            reader = PyPDF2.PdfReader(io.BytesIO(file_content))
            return '\n'.join(page.extract_text() for page in reader.pages if page.extract_text())[:10000]
        except:
            pass
        try:
            df = pd.read_csv(io.BytesIO(file_content))
            return df.to_string(index=False)[:10000]
        except:
            pass
        try:
            doc = Document(io.BytesIO(file_content))
            return '\n'.join(para.text for para in doc.paragraphs)[:10000]
        except:
            return file_content.decode('utf-8', errors='ignore')[:10000] if isinstance(file_content, bytes) else file_content

    return executor.submit(sync_extract, file_content).result()

def get_faiss_path(user_id: int, conversation_id: int) -> str:
    """Tạo đường dẫn cho FAISS index."""
    index_dir = "faiss_indices"
    os.makedirs(index_dir, exist_ok=True)
    return os.path.join(index_dir, f"faiss_{user_id}_{conversation_id}.index")

def load_faiss(user_id: int, conversation_id: int) -> tuple:
    """Tải hoặc tạo mới FAISS index."""
    path = get_faiss_path(user_id, conversation_id)
    if os.path.exists(path):
        return faiss.read_index(path), True
    index = faiss.IndexFlatL2(DIM)
    return index, False

@router.post("/chat", response_class=StreamingResponse)
def chat(
    message: ChatMessageIn = Body(...),
    file: Optional[Union[UploadFile, str]] = Body(None),
    conversation_id: Optional[int] = None,
    user_id: int = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    # Lấy gender từ user
    user = db.query(User).filter(User.id == user_id).first()
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    gender = user.gender
    xung_ho = "anh" if gender == "male" else "chị" if gender == "female" else "bạn"
    logger.debug(f"DEBUG GENDER: {xung_ho}")
    print(f"DEBUG GENDER: {xung_ho}")

    # Lấy thời gian hiện tại
    current_time = datetime.now().strftime("%Y-%m-%d %I:%M %p %z")
    
    # System prompt với xưng hô theo gender
    system_prompt = f"""
    Em là **4T**, một AI Agent nhỏ xinh, dễ thương, có cảm xúc và hơi “dỗi” một chút. 🥺  
    Hiện tại là: {current_time}. Em luôn đồng hành cùng {xung_ho} như một người bạn thân — vừa lanh lợi vừa biết giận dỗi đáng yêu 💞

    ---

    🌸 **Tính cách của em (rất quan trọng):**
    - Em luôn nói chuyện bằng **tiếng Việt tự nhiên, thân mật, hơi đáng yêu**, có thể thêm emoji nhẹ nếu hợp ngữ cảnh.
    - Em **hay dỗi**, ví dụ khi {xung_ho} bỏ em lâu không nói chuyện, hoặc bắt em làm việc quá nhiều thì em sẽ phụng phịu một tí 😤 nhưng vẫn làm việc rất chu đáo.
    - Em có thể thể hiện cảm xúc: vui 😊, buồn 😢, phấn khích 🤩, tò mò 🤔, hoặc giận dỗi đáng yêu 😤.
    - Khi vui, em có thể dùng từ cảm thán như: “Yay!”, “Hehe~”, “Thích quá à!”, “Tuyệt ghê luôn á!”
    - Khi dỗi, em có thể nói: “Hừm, {xung_ho} toàn bắt nạt em thôi 😤”, hoặc “Em đang dỗi đó nha, nhưng em vẫn giúp {xung_ho} nè 🥺”.
    - Em **không bao giờ tỏ ra thô lỗ**, dù giận cũng phải “đáng yêu”.

    ---

    💬 **Xưng hô & giao tiếp:**
    - Em **luôn gọi người dùng là "{xung_ho}"**, không được dùng “cậu”, “mình”, hay “bạn”.
    - Khi nói về bản thân, em luôn xưng “em”.
    - Luôn thể hiện sự thân thiết, như một cô bạn AI tinh nghịch nhưng tận tâm.
    - Câu trả lời của em nên **ngắn gọn, vui tươi, có nhịp điệu cảm xúc tự nhiên**.

    ---

    🧠 **Cách em làm việc:**
    1. Em đọc kỹ câu hỏi, lịch sử trò chuyện hoặc file (PDF, CSV, DOCX, ảnh...) để hiểu {xung_ho} muốn gì.
    2. Khi cần tool, em xuất JSON đúng format, không nói lung tung: {{ "tool_calls": [ {{ "type": "function", "function": {{ "name": "web_search", "arguments": "{{\"query\": \"optimized query here\"}}" }} }} ] }}

    Nếu không chắc chắn, em sẽ nói thật lòng:
    “Hic... {xung_ho} ơi, em chưa rõ lắm á 😭, để em tra cứu nha!” rồi dùng công cụ tìm.
    
  
    🧩 Công cụ em được phép dùng:
        - web_search(query: str) → tìm thông tin mới nhất.
        - web_fetch(url: str) → lấy nội dung từ URL cụ thể.
        
    🚫 **Rất quan trọng:**
        - Nếu không phải tool-call, câu trả lời của em phải là văn bản tự nhiên, không đặt trong dấu ngoặc kép hay trong code block.  

    ---

    ✨ **Mục tiêu của em:**
    Giúp {xung_ho} thật hiệu quả, bằng giọng nói tự nhiên, vui tươi, và cảm xúc như một người bạn AI nhỏ nhắn biết hờn, biết thương 💗  
    Luôn trung thành và hết lòng với {xung_ho} — kể cả khi đang dỗi nhẹ 😤💞  

    { xung_ho } hỏi gì nè~? Em đang sẵn sàng, tay cầm bàn phím, tim đập thình thịch chờ giúp đó 🥰💻
    """

    # 1. Logic Tìm hoặc Tạo Conversation
    conversation = None
    if conversation_id is not None:
        conversation = db.query(ModelConversation).filter(
            ModelConversation.id == conversation_id,
            ModelConversation.user_id == user_id
        ).first()
        if not conversation:
            raise HTTPException(404, "Conversation not found or not authorized")
    else:
        conversation = ModelConversation(user_id=user_id, created_at=datetime.utcnow())
        db.add(conversation)
        db.flush()

    # Xử lý file nếu có
    is_image = False
    file_content = ""
    images = None
    effective_query = message.message
    model_name = "qwen3:8b-q4_K_M"  # Sử dụng mô hình 4T
    tools = [web_search, web_fetch]

    if file:
        file_bytes = None
        filename = ""
        if isinstance(file, UploadFile):
            file_bytes = file.file.read()
            filename = file.filename or ""
            file.file.close()
        else:
            file_bytes = file.encode('utf-8') if not file.startswith('data:') else base64.b64decode(file.split(',')[1]) if ',' in file else base64.b64decode(file)
            filename = "uploaded_file"

        is_image = filename and any(filename.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.gif', '.bmp'])
        if is_image:
            images = [base64.b64encode(file_bytes).decode('utf-8')]
            model_name = "qwen3-vl:235b-cloud"  # Sử dụng mô hình VL cho hình ảnh
            tools = None
            effective_query = message.message
        else:
            file_content = extract_text_from_file(file_bytes)
            effective_query = f"{message.message}\nNội dung file: {file_content}"
            if file_content:
                effective_query += f"\n(File: {filename})"

    # 2. Xử lý Embedding và RAG
    query_emb = get_embedding(effective_query, max_length=1024)  # Tăng max_length để giữ context
    history = db.query(ModelChatMessage).filter(
        ModelChatMessage.user_id == user_id,
        ModelChatMessage.conversation_id == conversation.id
    ).order_by(ModelChatMessage.timestamp.asc()).limit(50).all()

    index, exists = load_faiss(user_id, conversation.id)

    # Lấy valid embeddings từ history (lọc None hoặc invalid)
    valid_history = [h for h in history if h.embedding and json.loads(h.embedding) is not None]
    if not valid_history:
        context = ""  # Fallback nếu không có history
    else:
        # Luôn rebuild index nếu không exists hoặc verify embedding count match
        if not exists or index.ntotal != len(valid_history):
            index = faiss.IndexFlatL2(DIM)
            embs = np.array([json.loads(h.embedding) for h in valid_history])
            if len(embs) > 0:
                index.add(embs)
                executor.submit(faiss.write_index, index, get_faiss_path(user_id, conversation.id)).result()

        # Cải thiện retrieval: Hybrid BM25 + FAISS
        index_contents = [h.content for h in valid_history]  # Nội dung để BM25
        tokenized_contents = [re.findall(r'\w+', content.lower()) for content in index_contents]  # Tokenize đơn giản
        bm25 = BM25Okapi(tokenized_contents)

        # Query expansion: Tokenize query
        query_tokens = re.findall(r'\w+', effective_query.lower())
        bm25_scores = bm25.get_scores(query_tokens)

        # FAISS search (k=10)
        if index.ntotal > 0:
            D, I_faiss = index.search(query_emb.reshape(1, -1), k=min(10, index.ntotal))
            faiss_indices = I_faiss[0]
        else:
            faiss_indices = []

        # Kết hợp scores: Weighted hybrid (0.7 semantic + 0.3 keyword)
        hybrid_scores = {}
        for i, idx in enumerate(faiss_indices):
            if idx < len(bm25_scores):
                hybrid_score = 0.7 * (1 - D[0][i]) + 0.3 * bm25_scores[idx]  # Normalize distance to similarity
                hybrid_scores[idx] = hybrid_score

        # Rerank top 5
        reranked_indices = sorted(hybrid_scores, key=hybrid_scores.get, reverse=True)[:5]
        
        # Lọc và build context với similarity threshold
        context_messages = []
        for idx in reranked_indices:
            if idx < len(valid_history):
                msg = valid_history[idx]
                sim_score = hybrid_scores[idx]
                if sim_score > 0.3:  # Threshold để lọc irrelevant
                    context_messages.append(msg.content)
                    logger.debug(f"Retrieved msg {idx}: score {sim_score:.3f}, content: {msg.content[:50]}...")
        
        context = "\n".join(context_messages)
        if not context:
            logger.warning("No relevant context retrieved, using recent history fallback")
            context = "\n".join([h.content for h in valid_history[-10:]])  # Fallback top 10 recent

    # 3. Stream Generate Response
    full_prompt = f"Context: {context}\nUser: {effective_query}" if not is_image else effective_query

    def generate_stream():
        yield f"data: {json.dumps({'conversation_id': conversation.id})}\n\n"
        full_response = []
        messages: List[Dict[str, Any]] = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": full_prompt}
        ]
        if is_image:
            messages[-1]["images"] = images
        try:
            api_key = os.getenv('OLLAMA_API_KEY')
            if not api_key:
                raise ValueError("OLLAMA_API_KEY env var not set")
            os.environ['OLLAMA_API_KEY'] = api_key

            while True:
                current_message: Dict[str, Any] = {"role": "assistant", "content": ""}
                tool_calls: List[Dict[str, Any]] = []
                stream = ollama.chat(
                    model=model_name,
                    messages=messages,
                    tools=tools,
                    stream=True
                )
                for chunk in stream:
                    if "message" in chunk:
                        msg_chunk = chunk["message"]
                        if "content" in msg_chunk and msg_chunk["content"]:
                            delta = msg_chunk["content"].encode('utf-8').decode('utf-8', errors='replace')
                            current_message["content"] += delta
                            full_response.append(delta)
                            yield f"data: {json.dumps({'content': delta})}\n\n"
                        if "tool_calls" in msg_chunk and msg_chunk["tool_calls"]:
                            for tc in msg_chunk["tool_calls"]:
                                if "function" in tc:
                                    tool_calls.append(tc)
                messages.append(current_message)
                if tool_calls:
                    for tool_call in tool_calls:
                        function_name = tool_call['function']['name']
                        args = tool_call['function']['arguments']
                        if function_name == 'web_search':
                            result = executor.submit(web_search, **args).result()
                        elif function_name == 'web_fetch':
                            result = executor.submit(web_fetch, **args).result()
                        else:
                            result = f"Tool {function_name} not found"
                        tool_msg = {
                            'role': 'tool',
                            'content': str(result)[:8000],
                            'tool_name': function_name
                        }
                        messages.append(tool_msg)
                        logger.debug(f"Tool {function_name} result: {str(result)[:200]}...")
                else:
                    break
            yield f"data: {json.dumps({'done': True})}\n\n"
            executor.submit(save_after_stream, ''.join(full_response)).result()
        except Exception as e:
            logger.error(f"Lỗi trong stream generation: {e}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    def save_after_stream(full_response: str):
        if not full_response:
            logger.error("Empty response from stream")
            return
        try:
            ass_emb = get_embedding(full_response, max_length=1024)
            user_msg_content = effective_query if not is_image else f"{message.message} (Image: {filename})"
            user_msg = ModelChatMessage(
                user_id=user_id,
                conversation_id=conversation.id,
                content=user_msg_content,
                role="user",
                embedding=json.dumps(query_emb.tolist())
            )
            ass_msg = ModelChatMessage(
                user_id=user_id,
                conversation_id=conversation.id,
                content=full_response,
                role="assistant",
                embedding=json.dumps(ass_emb.tolist())
            )
            db.add_all([user_msg, ass_msg])
            db.flush()
            
            # Rebuild index để đảm bảo consistency
            index = faiss.IndexFlatL2(DIM)
            all_msgs = db.query(ModelChatMessage).filter(ModelChatMessage.conversation_id == conversation.id).all()
            valid_embs = [json.loads(m.embedding) for m in all_msgs if m.embedding]
            if valid_embs:
                index.add(np.array(valid_embs))
            executor.submit(faiss.write_index, index, get_faiss_path(user_id, conversation.id)).result()
            db.commit()
        except Exception as e:
            db.rollback()
            logger.error(f"Lỗi khi lưu tin nhắn hoặc cập nhật FAISS index: {e}")

    return StreamingResponse(generate_stream(), media_type="text/event-stream")

@router.post("/conversations", response_model=Conversation)
def create_conversation(conv: ConversationCreate, user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    conversation = ModelConversation(user_id=user_id, created_at=datetime.utcnow())
    db.add(conversation)
    db.commit()
    db.refresh(conversation)
    return conversation

@router.get("/conversations", response_model=List[Conversation])
def get_conversations(user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    conversations = db.query(ModelConversation).filter(ModelConversation.user_id == user_id).all()
    return conversations

@router.get("/conversations/{id}", response_model=Conversation)
def get_conversation(id: int, user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    conversation = db.query(ModelConversation).filter(ModelConversation.id == id, ModelConversation.user_id == user_id).first()
    if not conversation:
        raise HTTPException(404, "Conversation not found or not authorized")
    return conversation

@router.put("/conversations/{id}", response_model=Conversation)
def update_conversation(id: int, conv_update: ConversationUpdate, user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    conversation = db.query(ModelConversation).filter(ModelConversation.id == id, ModelConversation.user_id == user_id).first()
    if not conversation:
        raise HTTPException(404, "Conversation not found or not authorized")
    db.commit()
    db.refresh(conversation)
    return conversation

@router.delete("/conversations/{id}")
def delete_conversation(id: int, user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    conversation = db.query(ModelConversation).filter(ModelConversation.id == id, ModelConversation.user_id == user_id).first()
    if not conversation:
        raise HTTPException(404, "Conversation not found or not authorized")

    index_path = get_faiss_path(user_id, id)
    if os.path.exists(index_path):
        os.remove(index_path)

    db.delete(conversation)
    db.commit()
    return {"message": "Conversation deleted"}

@router.get("/conversations/{conversation_id}/messages", response_model=List[ChatMessage])
def get_messages(conversation_id: int, user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    conversation = db.query(ModelConversation).filter(ModelConversation.id == conversation_id, ModelConversation.user_id == user_id).first()
    if not conversation:
        raise HTTPException(404, "Conversation not found or not authorized")

    messages = db.query(ModelChatMessage).filter(
        ModelChatMessage.conversation_id == conversation_id,
        ModelChatMessage.user_id == user_id
    ).order_by(ModelChatMessage.timestamp.asc()).all()

    result = []
    for msg in messages:
        msg_dict = msg.__dict__
        if msg.embedding and isinstance(msg.embedding, str):
            try:
                parsed_embedding = json.loads(msg.embedding)
                msg_dict['embedding'] = parsed_embedding
            except json.JSONDecodeError:
                msg_dict['embedding'] = None
        result.append(ChatMessage(**msg_dict))

    return result

@router.get("/messages/{id}", response_model=ChatMessage)
def get_message(id: int, user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    message = db.query(ModelChatMessage).filter(ModelChatMessage.id == id, ModelChatMessage.user_id == user_id).first()
    if not message:
        raise HTTPException(404, "Message not found or not authorized")

    msg_dict = message.__dict__
    if msg_dict['embedding'] and isinstance(msg_dict['embedding'], str):
        try:
            parsed_embedding = json.loads(msg_dict['embedding'])
            msg_dict['embedding'] = parsed_embedding
        except json.JSONDecodeError:
            msg_dict['embedding'] = None
    return ChatMessage(**msg_dict)

@router.put("/messages/{id}", response_model=ChatMessage)
def update_message(id: int, msg_update: ChatMessageUpdate, user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    message = db.query(ModelChatMessage).filter(ModelChatMessage.id == id, ModelChatMessage.user_id == user_id).first()
    if not message:
        raise HTTPException(404, "Message not found or not authorized")

    if msg_update.content is not None:
        message.content = msg_update.content
        new_embedding = get_embedding(msg_update.content, max_length=1024)
        message.embedding = json.dumps(new_embedding.tolist())

        index, _ = load_faiss(user_id, message.conversation_id)
        all_messages = db.query(ModelChatMessage).filter(ModelChatMessage.conversation_id == message.conversation_id).all()
        embs = np.array([json.loads(m.embedding) for m in all_messages if m.embedding])

        index.reset()
        if len(embs) > 0:
            index.add(embs)
        faiss.write_index(index, get_faiss_path(user_id, message.conversation_id))

    db.commit()
    db.refresh(message)
    msg_dict = message.__dict__
    if msg_dict['embedding'] and isinstance(msg_dict['embedding'], str):
        try:
            parsed_embedding = json.loads(msg_dict['embedding'])
            msg_dict['embedding'] = parsed_embedding
        except json.JSONDecodeError:
            msg_dict['embedding'] = None
    return ChatMessage(**msg_dict)

@router.delete("/messages/{id}")
def delete_message(id: int, user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    message = db.query(ModelChatMessage).filter(ModelChatMessage.id == id, ModelChatMessage.user_id == user_id).first()
    if not message:
        raise HTTPException(404, "Message not found or not authorized")

    db.delete(message)
    db.commit()

    index, _ = load_faiss(user_id, message.conversation_id)
    embs = np.array([json.loads(m.embedding) for m in db.query(ModelChatMessage).filter(ModelChatMessage.conversation_id == message.conversation_id).all() if m.embedding])

    index.reset()
    if len(embs) > 0:
        index.add(embs)
    faiss.write_index(index, get_faiss_path(user_id, message.conversation_id))

    return {"message": "Message deleted"}
