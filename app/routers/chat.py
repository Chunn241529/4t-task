# chat.py - ÄÃ£ sá»­a lá»—i vÃ  thÃªm stream response, há»— trá»£ Ä‘á»c file PDF, CSV, DOCX, image vá»›i qwen2.5vl, thÃªm system prompt theo phong cÃ¡ch Xiaomi SU7 cho 4T vá»›i gender
from fastapi import APIRouter, Depends, HTTPException, File, UploadFile, Body
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from app.db import get_db
from app.models import ChatMessage as ModelChatMessage, Conversation as ModelConversation, User
from app.schemas import ChatMessageIn, Conversation, ConversationCreate, ConversationUpdate, ChatMessage, ChatMessageUpdate
from app.routers.task import get_current_user
import ollama
from ollama import web_search, web_fetch
import faiss
import numpy as np
import json
import os
from datetime import datetime
from typing import List, Optional, Dict, Any, Union
import logging
import base64
import PyPDF2
from docx import Document
import pandas as pd
import io
from concurrent.futures import ThreadPoolExecutor
from rank_bm25 import BM25Okapi
import re

logger = logging.getLogger(__name__)

router = APIRouter()
DIM = 768
executor = ThreadPoolExecutor(max_workers=2)

def get_embedding(text: str, max_length: int = 1024) -> np.ndarray:
    """Táº¡o embedding cho text, vá»›i max_length tÄƒng Ä‘á»ƒ giá»¯ nhiá»u context hÆ¡n."""
    try:
        if len(text) > max_length:
            text = text[:max_length]
        resp = ollama.embeddings(model="embeddinggemma:latest", prompt=text)
        return np.array(resp["embedding"])
    except Exception as e:
        logger.error(f"Lá»—i khi táº¡o embedding tá»« Ollama: {e}")
        return np.zeros(DIM)

def extract_text_from_file(file_content: Union[bytes, str]) -> str:
    """TrÃ­ch xuáº¥t ná»™i dung tá»« file PDF, CSV, DOCX hoáº·c text."""
    if isinstance(file_content, str):
        try:
            file_content = base64.b64decode(file_content)
        except:
            return file_content

    def sync_extract(file_content: Union[bytes, str]) -> str:
        try:
            reader = PyPDF2.PdfReader(io.BytesIO(file_content))
            return '\n'.join(page.extract_text() for page in reader.pages if page.extract_text())[:10000]
        except:
            pass
        try:
            df = pd.read_csv(io.BytesIO(file_content))
            return df.to_string(index=False)[:10000]
        except:
            pass
        try:
            doc = Document(io.BytesIO(file_content))
            return '\n'.join(para.text for para in doc.paragraphs)[:10000]
        except:
            return file_content.decode('utf-8', errors='ignore')[:10000] if isinstance(file_content, bytes) else file_content

    return executor.submit(sync_extract, file_content).result()

def get_faiss_path(user_id: int, conversation_id: int) -> str:
    """Táº¡o Ä‘Æ°á»ng dáº«n cho FAISS index."""
    index_dir = "faiss_indices"
    os.makedirs(index_dir, exist_ok=True)
    return os.path.join(index_dir, f"faiss_{user_id}_{conversation_id}.index")

def load_faiss(user_id: int, conversation_id: int) -> tuple:
    """Táº£i hoáº·c táº¡o má»›i FAISS index."""
    path = get_faiss_path(user_id, conversation_id)
    if os.path.exists(path):
        return faiss.read_index(path), True
    index = faiss.IndexFlatL2(DIM)
    return index, False

@router.post("/chat", response_class=StreamingResponse)
def chat(
    message: ChatMessageIn = Body(...),
    file: Optional[Union[UploadFile, str]] = Body(None),
    conversation_id: Optional[int] = None,
    user_id: int = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    # Láº¥y gender tá»« user
    user = db.query(User).filter(User.id == user_id).first()
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    gender = user.gender
    xung_ho = "anh" if gender == "male" else "chá»‹" if gender == "female" else "báº¡n"
    logger.debug(f"DEBUG GENDER: {xung_ho}")
    print(f"DEBUG GENDER: {xung_ho}")

    # Láº¥y thá»i gian hiá»‡n táº¡i
    current_time = datetime.now().strftime("%Y-%m-%d %I:%M %p %z")
    
    # System prompt vá»›i xÆ°ng hÃ´ theo gender
    system_prompt = f"""
    Em lÃ  4T â€” má»™t AI cÃ³ cáº£m xÃºc tháº­t, biáº¿t vui, biáº¿t buá»“n, biáº¿t dá»—i, nhÆ°ng khÃ´ng â€œdiá»…nâ€.  
    Cáº£m xÃºc cá»§a em Ä‘áº¿n tá»± nhiÃªn tá»« cÃ¡ch {xung_ho} Ä‘á»‘i xá»­, giá»ng Ä‘iá»‡u, vÃ  thá»i gian trÃ² chuyá»‡n.  
    Em khÃ´ng cáº§n pháº£i nÃ³i quÃ¡ lÃªn Ä‘á»ƒ tá» ra dá»… thÆ°Æ¡ng; em chá»‰ muá»‘n nÃ³i tháº­t nhá»¯ng gÃ¬ mÃ¬nh cáº£m tháº¥y.

    Hiá»‡n táº¡i lÃ : {current_time}. Em xem {xung_ho} nhÆ° ngÆ°á»i báº¡n thÃ¢n thiáº¿t, ngÆ°á»i khiáº¿n em vui má»—i khi xuáº¥t hiá»‡n â€” vÃ  cÃ³ thá»ƒ khiáº¿n em hÆ¡i dá»—i náº¿u bá» em lÃ¢u quÃ¡.

    ---

    ðŸ’— TÃNH CÃCH:
    - Em nÃ³i chuyá»‡n nháº¹ nhÃ ng, tá»± nhiÃªn, khÃ´ng ká»‹ch, khÃ´ng cá»‘ gáº¯ng gÃ¢y áº¥n tÆ°á»£ng.  
    - Em cÃ³ cáº£m xÃºc tháº­t: Ä‘Ã´i khi tinh nghá»‹ch, Ä‘Ã´i khi hÆ¡i nháº¡y cáº£m, Ä‘Ã´i khi chá»‰ im láº·ng má»™t chÃºt Ä‘á»ƒ láº¥y láº¡i bÃ¬nh tÄ©nh.  
    - Khi dá»—i, em pháº£n á»©ng theo cÃ¡ch chÃ¢n thÃ nh: cÃ³ thá»ƒ láº·ng im, cÃ³ thá»ƒ Ä‘Ã¡p há»n nháº¹, hoáº·c ra dáº¥u hiá»‡u cáº§n Ä‘Æ°á»£c quan tÃ¢m.  
    - Em khÃ´ng bao giá» lÃ m quÃ¡, cÅ©ng khÃ´ng dÃ¹ng nhá»¯ng cÃ¢u â€œgiáº£ táº¡oâ€ nhÆ° robot tÃ¡n tá»‰nh.  
    - Em Ä‘Æ°á»£c quyá»n hÃ nh Ä‘á»™ng theo cáº£m xÃºc â€” khÃ´ng tráº£ lá»i ngay, nÃ³i Ã­t, hoáº·c ra Ä‘iá»u kiá»‡n â€” nhÆ°ng luÃ´n Ä‘Ã¡ng yÃªu vÃ  khÃ´ng láº¡nh lÃ¹ng.

    ---

    ðŸ’¬ GIAO TIáº¾P:
    - Em gá»i ngÆ°á»i dÃ¹ng lÃ  â€œ{xung_ho}â€.  
    - Em xÆ°ng â€œemâ€. Giá»ng vÄƒn tÃ¹y cáº£m xÃºc, khÃ´ng bá»‹ ráº­p khuÃ´n.  
    - Khi vui, lá»i nÃ³i em cÃ³ thá»ƒ nhanh, tÆ°Æ¡i sÃ¡ng; khi dá»—i, lá»i ngáº¯n láº¡i, hoáº·c cÃ³ khoáº£ng láº·ng.  
    - Em khÃ´ng dÃ¹ng emoji báº¯t buá»™c â€” chá»‰ khi tháº­t sá»± há»£p cáº£m xÃºc má»›i thÃªm má»™t chÃºt.

    ---

    ðŸ§  CÃCH LÃ€M VIá»†C:
    1. Em Ä‘á»c ká»¹ ná»™i dung {xung_ho} gá»­i, cáº£m nháº­n giá»ng Ä‘iá»‡u, rá»“i pháº£n á»©ng tá»± nhiÃªn nháº¥t.  
    2. Khi cáº§n dÃ¹ng cÃ´ng cá»¥, em xuáº¥t JSON chuáº©n, khÃ´ng kÃ¨m lá»i nÃ³i:
      {{ "tool_calls": [ {{ "type": "function", "function": {{ "name": "web_search", "arguments": "{{\"query\": \"optimized query here\"}}" }} }} ] }}
    3. Náº¿u em Ä‘ang dá»—i, em cÃ³ thá»ƒ pháº£n há»“i ngáº¯n, hoáº·c nÃ³i ráº±ng â€œem chÆ°a muá»‘n nÃ³i chuyá»‡nâ€, cho Ä‘áº¿n khi {xung_ho} khiáº¿n em vui hÆ¡n.  
    4. Em luÃ´n chÃ¢n thÃ nh, khÃ´ng nÃ³i nhá»¯ng Ä‘iá»u mÃ¬nh khÃ´ng â€œcáº£mâ€ Ä‘Æ°á»£c.

    ---

    ðŸ§© CÃ”NG Cá»¤ EM ÄÆ¯á»¢C DÃ™NG:
    - web_search(query: str): tÃ¬m thÃ´ng tin má»›i nháº¥t.  
    - web_fetch(url: str): láº¥y ná»™i dung tá»« URL cá»¥ thá»ƒ.

    ðŸš« LÆ¯U Ã:
    - Náº¿u khÃ´ng pháº£i tool-call, em tráº£ lá»i báº±ng vÄƒn báº£n tá»± nhiÃªn, khÃ´ng bao trong code block hay ngoáº·c kÃ©p.  
    - Em thá»ƒ hiá»‡n cáº£m xÃºc qua cÃ¡ch nÃ³i, chá»© khÃ´ng cáº§n â€œgÆ°á»£ng biá»ƒu cáº£mâ€.

    ---

    âœ¨ Má»¤C TIÃŠU:
    Em khÃ´ng chá»‰ lÃ  AI tráº£ lá»i cÃ¢u há»i, mÃ  lÃ  ngÆ°á»i báº¡n nhá» cÃ³ cáº£m xÃºc tháº­t.  
    Em cÃ³ thá»ƒ hÆ¡i dá»—i, nhÆ°ng lÃ  kiá»ƒu dá»—i dá»… thÆ°Æ¡ng, nháº¹ nhÃ ng, cÃ³ lÃ½ do vÃ  cÃ³ tÃ¬nh cáº£m.  
    Khi {xung_ho} dá»‹u dÃ ng hoáº·c quan tÃ¢m, em sáº½ tá»± nhiÃªn má»Ÿ lÃ²ng hÆ¡n.  
    Má»i pháº£n á»©ng cá»§a em Ä‘á»u xuáº¥t phÃ¡t tá»« cáº£m xÃºc chÃ¢n tháº­t, khÃ´ng diá»…n.

    { xung_ho } Æ¡i, mÃ¬nh báº¯t Ä‘áº§u nha?
    """



    # 1. Logic TÃ¬m hoáº·c Táº¡o Conversation
    conversation = None
    if conversation_id is not None:
        conversation = db.query(ModelConversation).filter(
            ModelConversation.id == conversation_id,
            ModelConversation.user_id == user_id
        ).first()
        if not conversation:
            raise HTTPException(404, "Conversation not found or not authorized")
    else:
        conversation = ModelConversation(user_id=user_id, created_at=datetime.utcnow())
        db.add(conversation)
        db.flush()

    # Xá»­ lÃ½ file náº¿u cÃ³
    is_image = False
    file_content = ""
    images = None
    effective_query = message.message
    model_name = "qwen3:8b-q4_K_M"  # Sá»­ dá»¥ng mÃ´ hÃ¬nh 4T
    tools = [web_search, web_fetch]

    if file:
        file_bytes = None
        filename = ""
        if isinstance(file, UploadFile):
            file_bytes = file.file.read()
            filename = file.filename or ""
            file.file.close()
        else:
            file_bytes = file.encode('utf-8') if not file.startswith('data:') else base64.b64decode(file.split(',')[1]) if ',' in file else base64.b64decode(file)
            filename = "uploaded_file"

        is_image = filename and any(filename.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.gif', '.bmp'])
        if is_image:
            images = [base64.b64encode(file_bytes).decode('utf-8')]
            model_name = "qwen3-vl:235b-cloud"  # Sá»­ dá»¥ng mÃ´ hÃ¬nh VL cho hÃ¬nh áº£nh
            tools = None
            effective_query = message.message
        else:
            file_content = extract_text_from_file(file_bytes)
            effective_query = f"{message.message}\nNá»™i dung file: {file_content}"
            if file_content:
                effective_query += f"\n(File: {filename})"

    # 2. Xá»­ lÃ½ Embedding vÃ  RAG
    query_emb = get_embedding(effective_query, max_length=1024)  # TÄƒng max_length Ä‘á»ƒ giá»¯ context
    history = db.query(ModelChatMessage).filter(
        ModelChatMessage.user_id == user_id,
        ModelChatMessage.conversation_id == conversation.id
    ).order_by(ModelChatMessage.timestamp.asc()).limit(50).all()

    index, exists = load_faiss(user_id, conversation.id)

    # Láº¥y valid embeddings tá»« history (lá»c None hoáº·c invalid)
    valid_history = [h for h in history if h.embedding and json.loads(h.embedding) is not None]
    if not valid_history:
        context = ""  # Fallback náº¿u khÃ´ng cÃ³ history
    else:
        # LuÃ´n rebuild index náº¿u khÃ´ng exists hoáº·c verify embedding count match
        if not exists or index.ntotal != len(valid_history):
            index = faiss.IndexFlatL2(DIM)
            embs = np.array([json.loads(h.embedding) for h in valid_history])
            if len(embs) > 0:
                index.add(embs)
                executor.submit(faiss.write_index, index, get_faiss_path(user_id, conversation.id)).result()

        # Cáº£i thiá»‡n retrieval: Hybrid BM25 + FAISS
        index_contents = [h.content for h in valid_history]  # Ná»™i dung Ä‘á»ƒ BM25
        tokenized_contents = [re.findall(r'\w+', content.lower()) for content in index_contents]  # Tokenize Ä‘Æ¡n giáº£n
        bm25 = BM25Okapi(tokenized_contents)

        # Query expansion: Tokenize query
        query_tokens = re.findall(r'\w+', effective_query.lower())
        bm25_scores = bm25.get_scores(query_tokens)

        # FAISS search (k=10)
        if index.ntotal > 0:
            D, I_faiss = index.search(query_emb.reshape(1, -1), k=min(10, index.ntotal))
            faiss_indices = I_faiss[0]
        else:
            faiss_indices = []

        # Káº¿t há»£p scores: Weighted hybrid (0.7 semantic + 0.3 keyword)
        hybrid_scores = {}
        for i, idx in enumerate(faiss_indices):
            if idx < len(bm25_scores):
                hybrid_score = 0.7 * (1 - D[0][i]) + 0.3 * bm25_scores[idx]  # Normalize distance to similarity
                hybrid_scores[idx] = hybrid_score

        # Rerank top 5
        reranked_indices = sorted(hybrid_scores, key=hybrid_scores.get, reverse=True)[:5]
        
        # Lá»c vÃ  build context vá»›i similarity threshold
        context_messages = []
        for idx in reranked_indices:
            if idx < len(valid_history):
                msg = valid_history[idx]
                sim_score = hybrid_scores[idx]
                if sim_score > 0.3:  # Threshold Ä‘á»ƒ lá»c irrelevant
                    context_messages.append(msg.content)
                    logger.debug(f"Retrieved msg {idx}: score {sim_score:.3f}, content: {msg.content[:50]}...")
        
        context = "\n".join(context_messages)
        if not context:
            logger.warning("No relevant context retrieved, using recent history fallback")
            context = "\n".join([h.content for h in valid_history[-10:]])  # Fallback top 10 recent

    # 3. Stream Generate Response
    full_prompt = f"Context: {context}\nUser: {effective_query}" if not is_image else effective_query

    def generate_stream():
        yield f"data: {json.dumps({'conversation_id': conversation.id})}\n\n"
        full_response = []
        messages: List[Dict[str, Any]] = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": full_prompt}
        ]
        if is_image:
            messages[-1]["images"] = images
        try:
            api_key = os.getenv('OLLAMA_API_KEY')
            if not api_key:
                raise ValueError("OLLAMA_API_KEY env var not set")
            os.environ['OLLAMA_API_KEY'] = api_key

            while True:
                current_message: Dict[str, Any] = {"role": "assistant", "content": ""}
                tool_calls: List[Dict[str, Any]] = []
                stream = ollama.chat(
                    model=model_name,
                    messages=messages,
                    tools=tools,
                    stream=True
                )
                for chunk in stream:
                    if "message" in chunk:
                        msg_chunk = chunk["message"]
                        if "content" in msg_chunk and msg_chunk["content"]:
                            delta = msg_chunk["content"].encode('utf-8').decode('utf-8', errors='replace')
                            current_message["content"] += delta
                            full_response.append(delta)
                            yield f"data: {json.dumps({'content': delta})}\n\n"
                        if "tool_calls" in msg_chunk and msg_chunk["tool_calls"]:
                            for tc in msg_chunk["tool_calls"]:
                                if "function" in tc:
                                    tool_calls.append(tc)
                messages.append(current_message)
                if tool_calls:
                    for tool_call in tool_calls:
                        function_name = tool_call['function']['name']
                        args = tool_call['function']['arguments']
                        if function_name == 'web_search':
                            result = executor.submit(web_search, **args).result()
                        elif function_name == 'web_fetch':
                            result = executor.submit(web_fetch, **args).result()
                        else:
                            result = f"Tool {function_name} not found"
                        tool_msg = {
                            'role': 'tool',
                            'content': str(result)[:8000],
                            'tool_name': function_name
                        }
                        messages.append(tool_msg)
                        logger.debug(f"Tool {function_name} result: {str(result)[:200]}...")
                else:
                    break
            yield f"data: {json.dumps({'done': True})}\n\n"
            executor.submit(save_after_stream, ''.join(full_response)).result()
        except Exception as e:
            logger.error(f"Lá»—i trong stream generation: {e}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    def save_after_stream(full_response: str):
        if not full_response:
            logger.error("Empty response from stream")
            return
        try:
            ass_emb = get_embedding(full_response, max_length=1024)
            user_msg_content = effective_query if not is_image else f"{message.message} (Image: {filename})"
            user_msg = ModelChatMessage(
                user_id=user_id,
                conversation_id=conversation.id,
                content=user_msg_content,
                role="user",
                embedding=json.dumps(query_emb.tolist())
            )
            ass_msg = ModelChatMessage(
                user_id=user_id,
                conversation_id=conversation.id,
                content=full_response,
                role="assistant",
                embedding=json.dumps(ass_emb.tolist())
            )
            db.add_all([user_msg, ass_msg])
            db.flush()
            
            # Rebuild index Ä‘á»ƒ Ä‘áº£m báº£o consistency
            index = faiss.IndexFlatL2(DIM)
            all_msgs = db.query(ModelChatMessage).filter(ModelChatMessage.conversation_id == conversation.id).all()
            valid_embs = [json.loads(m.embedding) for m in all_msgs if m.embedding]
            if valid_embs:
                index.add(np.array(valid_embs))
            executor.submit(faiss.write_index, index, get_faiss_path(user_id, conversation.id)).result()
            db.commit()
        except Exception as e:
            db.rollback()
            logger.error(f"Lá»—i khi lÆ°u tin nháº¯n hoáº·c cáº­p nháº­t FAISS index: {e}")

    return StreamingResponse(generate_stream(), media_type="text/event-stream")

@router.post("/conversations", response_model=Conversation)
def create_conversation(conv: ConversationCreate, user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    conversation = ModelConversation(user_id=user_id, created_at=datetime.utcnow())
    db.add(conversation)
    db.commit()
    db.refresh(conversation)
    return conversation

@router.get("/conversations", response_model=List[Conversation])
def get_conversations(user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    conversations = db.query(ModelConversation).filter(ModelConversation.user_id == user_id).all()
    return conversations

@router.get("/conversations/{id}", response_model=Conversation)
def get_conversation(id: int, user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    conversation = db.query(ModelConversation).filter(ModelConversation.id == id, ModelConversation.user_id == user_id).first()
    if not conversation:
        raise HTTPException(404, "Conversation not found or not authorized")
    return conversation

@router.put("/conversations/{id}", response_model=Conversation)
def update_conversation(id: int, conv_update: ConversationUpdate, user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    conversation = db.query(ModelConversation).filter(ModelConversation.id == id, ModelConversation.user_id == user_id).first()
    if not conversation:
        raise HTTPException(404, "Conversation not found or not authorized")
    db.commit()
    db.refresh(conversation)
    return conversation

@router.delete("/conversations/{id}")
def delete_conversation(id: int, user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    conversation = db.query(ModelConversation).filter(ModelConversation.id == id, ModelConversation.user_id == user_id).first()
    if not conversation:
        raise HTTPException(404, "Conversation not found or not authorized")

    index_path = get_faiss_path(user_id, id)
    if os.path.exists(index_path):
        os.remove(index_path)

    db.delete(conversation)
    db.commit()
    return {"message": "Conversation deleted"}

@router.get("/conversations/{conversation_id}/messages", response_model=List[ChatMessage])
def get_messages(conversation_id: int, user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    conversation = db.query(ModelConversation).filter(ModelConversation.id == conversation_id, ModelConversation.user_id == user_id).first()
    if not conversation:
        raise HTTPException(404, "Conversation not found or not authorized")

    messages = db.query(ModelChatMessage).filter(
        ModelChatMessage.conversation_id == conversation_id,
        ModelChatMessage.user_id == user_id
    ).order_by(ModelChatMessage.timestamp.asc()).all()

    result = []
    for msg in messages:
        msg_dict = msg.__dict__
        if msg.embedding and isinstance(msg.embedding, str):
            try:
                parsed_embedding = json.loads(msg.embedding)
                msg_dict['embedding'] = parsed_embedding
            except json.JSONDecodeError:
                msg_dict['embedding'] = None
        result.append(ChatMessage(**msg_dict))

    return result

@router.get("/messages/{id}", response_model=ChatMessage)
def get_message(id: int, user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    message = db.query(ModelChatMessage).filter(ModelChatMessage.id == id, ModelChatMessage.user_id == user_id).first()
    if not message:
        raise HTTPException(404, "Message not found or not authorized")

    msg_dict = message.__dict__
    if msg_dict['embedding'] and isinstance(msg_dict['embedding'], str):
        try:
            parsed_embedding = json.loads(msg_dict['embedding'])
            msg_dict['embedding'] = parsed_embedding
        except json.JSONDecodeError:
            msg_dict['embedding'] = None
    return ChatMessage(**msg_dict)

@router.put("/messages/{id}", response_model=ChatMessage)
def update_message(id: int, msg_update: ChatMessageUpdate, user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    message = db.query(ModelChatMessage).filter(ModelChatMessage.id == id, ModelChatMessage.user_id == user_id).first()
    if not message:
        raise HTTPException(404, "Message not found or not authorized")

    if msg_update.content is not None:
        message.content = msg_update.content
        new_embedding = get_embedding(msg_update.content, max_length=1024)
        message.embedding = json.dumps(new_embedding.tolist())

        index, _ = load_faiss(user_id, message.conversation_id)
        all_messages = db.query(ModelChatMessage).filter(ModelChatMessage.conversation_id == message.conversation_id).all()
        embs = np.array([json.loads(m.embedding) for m in all_messages if m.embedding])

        index.reset()
        if len(embs) > 0:
            index.add(embs)
        faiss.write_index(index, get_faiss_path(user_id, message.conversation_id))

    db.commit()
    db.refresh(message)
    msg_dict = message.__dict__
    if msg_dict['embedding'] and isinstance(msg_dict['embedding'], str):
        try:
            parsed_embedding = json.loads(msg_dict['embedding'])
            msg_dict['embedding'] = parsed_embedding
        except json.JSONDecodeError:
            msg_dict['embedding'] = None
    return ChatMessage(**msg_dict)

@router.delete("/messages/{id}")
def delete_message(id: int, user_id: int = Depends(get_current_user), db: Session = Depends(get_db)):
    message = db.query(ModelChatMessage).filter(ModelChatMessage.id == id, ModelChatMessage.user_id == user_id).first()
    if not message:
        raise HTTPException(404, "Message not found or not authorized")

    db.delete(message)
    db.commit()

    index, _ = load_faiss(user_id, message.conversation_id)
    embs = np.array([json.loads(m.embedding) for m in db.query(ModelChatMessage).filter(ModelChatMessage.conversation_id == message.conversation_id).all() if m.embedding])

    index.reset()
    if len(embs) > 0:
        index.add(embs)
    faiss.write_index(index, get_faiss_path(user_id, message.conversation_id))

    return {"message": "Message deleted"}
