import ollama

def test_ollama_think_low():
    """
    Test gpt-oss:20b vá»›i think="low" (táº¯t thinking gáº§n hoÃ n toÃ n, stream nhanh).
    """
    messages = [
        {"role": "user", "content": "Táº¡i sao báº§u trá»i mÃ u xanh?"}
    ]

    try:
        print("âœ… **Báº¯t Ä‘áº§u stream vá»›i think=low...**")
        print("\nğŸ“ **Response (nhanh, khÃ´ng thinking sÃ¢u):**\n")

        stream = ollama.chat(
            model="gpt-oss:20b",
            messages=messages,
            stream=True,
            think="low"  # â† Key fix: DÃ¹ng string "low" thay vÃ¬ False
        )

        full_response = ""
        for chunk in stream:
            content = chunk['message']['content']
            if content:  # Bá» qua chunk rá»—ng
                print(content, end="", flush=True)
                full_response += content

        print(f"\n\n{'='*50}")
        print("âœ… **Stream hoÃ n táº¥t nhanh!**")
        print(f"ğŸ”¢ Äá»™ dÃ i: {len(full_response)} kÃ½ tá»±")
        print(f"â±ï¸ Æ¯á»›c tÃ­nh: Ãt delay hÆ¡n so vá»›i medium")

    except Exception as e:
        print(f"\nâŒ **Lá»—i:** {e}")
        print("ğŸ’¡ Kiá»ƒm tra: ollama serve cháº¡y? Model pull? Thá»­ think='medium' Ä‘á»ƒ so sÃ¡nh.")

# Cháº¡y
if __name__ == "__main__":
    test_ollama_think_low()
